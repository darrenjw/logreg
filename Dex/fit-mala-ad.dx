'# MALA using Dex with automatic differentiation of gradients

-- load some generic utility functions
import djwutils

'## now read and process the data

dat = unsafe_io do read_file "../pima.data"
(AsList _ tab) = parse_tsv ' ' dat
atab = map (cons "1.0") tab
att = map (\r. list2tab r :: (Fin 9)=>String) atab
xStr = map (\r. slice r 0 (Fin 8)) att
xmb = map (\r. map parseString r) xStr :: (Fin 200)=>(Fin 8)=>(Maybe Float)
x = map (\r. map from_just r) xmb :: (Fin 200)=>(Fin 8)=>Float
yStrM = map (\r. slice r 8 (Fin 1)) att
yStr = (transpose yStrM).(0@_)
y = map (\s. select (s == "Yes") 1.0 0.0) yStr

x
y

'## now set up for MCMC

def ll (b: (Fin 8)=>Float) : Float =
  neg $ sum (log (map (\ x. (exp x) + 1) ((map (\ yi. 1 - 2*yi) y)*(x **. b))))

pscale = [10.0, 1, 1, 1, 1, 1, 1, 1] -- prior SDs
prscale = map (\ x. 1.0/x) pscale

def lprior (b: (Fin 8)=>Float) : Float =
  bs = b*prscale
  neg $ sum ((log pscale) + (0.5 .* (bs*bs)))

def lpost (b: (Fin 8)=>Float) : Float =
  (ll b) + (lprior b)

k = new_key 42

def mhKernel {s} (lpost: s -> Float) (rprop: Key -> s -> s) (dprop: s -> s -> Float) :
    Key -> (s & Float) -> (s & Float) =
  def kern (k: Key) (sll: (s & Float)) : (s & Float) =
    (x0, ll0) = sll
    [k1, k2] = split_key k
    x = rprop k1 x0
    ll = lpost x
    a = ll - ll0 + (dprop x0 x) - (dprop x x0)
    u = rand k2
    select (log u < a) (x, ll) (x0, ll0)
  kern

def malaKernel {n} (lpi: (Fin n)=>Float -> Float)
    (pre: (Fin n)=>Float) (dt: Float) :
    Key -> ((Fin n)=>Float & Float) -> ((Fin n)=>Float & Float) =
  sdt = sqrt dt
  spre = sqrt pre
  glp = grad lpi
  v = dt .* pre
  vinv = map (\ x. 1.0/x) v
  def advance (beta: (Fin n)=>Float) : (Fin n)=>Float =
    beta + (0.5*dt) .* (pre*(glp beta))
  def rprop (k: Key) (beta: (Fin n)=>Float) : (Fin n)=>Float =
    (advance beta) + sdt .* (spre*(randn_vec k))
  def dprop (new: (Fin n)=>Float) (old: (Fin n)=>Float) : Float =
    ao = advance old
    diff = new - ao
    -0.5 * sum ((log v) + diff*diff*vinv)
  mhKernel lpi rprop dprop

pre = [100.0,1,1,1,1,1,25,1] -- diagonal pre-conditioner

kern = malaKernel lpost pre 1.0e-5

init = [-9.0,0,0,0,0,0,0,0]

out = markov_chain k (init, -1.0e50) (step_n 1000 kern) 10000

mat = map fst out -- ditch log-posterior evaluations
mv = meanAndCovariance mat
fst mv -- mean
snd mv -- (co)variance matrix

unsafe_io do write_file "fit-mala-ad.tsv" (to_tsv mat)


-- eof
